{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYlPUuApdPvc"
   },
   "source": [
    "# **DIMEMEX â€” Complete Project Pipeline Notebook**\n",
    "\n",
    "*Multilingual Meme Translation & Hate Speech Analysis*\n",
    "\n",
    "---\n",
    "\n",
    "## **Members**\n",
    "\n",
    "* **BÃ¡rbara** (Text)\n",
    "* **Amanda** (Text + Description)\n",
    "* **Juan David Nieto** (Text + Description + Image)\n",
    "* **Luisa** (Image)\n",
    "\n",
    "---\n",
    "\n",
    "## **Project Overview**\n",
    "\n",
    "This project analyzes whether **offensive or hate speech content in memes is preserved after translating the memes from Spanish to Portuguese**.\n",
    "\n",
    "We work with the **DIMEMEX** dataset, which contains:\n",
    "\n",
    "* Meme **text**\n",
    "* Meme **description**\n",
    "* Meme **image**\n",
    "* Labels: *hate speech*, *inappropriate content*, *neither*\n",
    "\n",
    "This project has **two main tasks**:\n",
    "\n",
    "### **Task 1 â€” Translation Quality Evaluation**\n",
    "\n",
    "Translate the Spanish text to Portuguese and evaluate translation quality using standard NLP metrics:\n",
    "\n",
    "* **BLEURT**\n",
    "* **BERTScore**\n",
    "* **COMET-Kiwi**\n",
    "* **chrF**\n",
    "\n",
    "### **Task 2 â€” Hate Speech Detection (Multimodal Fine-Tuning)**\n",
    "\n",
    "Fine-tune models to classify:\n",
    "\n",
    "* Hate speech\n",
    "* Inappropriate content\n",
    "* Neither\n",
    "\n",
    "We fine-tune under four input settings:\n",
    "\n",
    "1. Text\n",
    "2. Text + Description\n",
    "3. Text + Description + Image (Multimodal)\n",
    "4. Image\n",
    "\n",
    "---\n",
    "\n",
    "## **ðŸ“Œ Objectives**\n",
    "\n",
    "1. Evaluate whether offensive content is **maintained or lost** during translation.\n",
    "2. Compare performance of **original Spanish memes vs. translated Portuguese memes**.\n",
    "3. Train and evaluate multimodal detectors to classify hate speech.\n",
    "4. Analyze cases where the label changes across languages.\n",
    "5. Perform **human qualitative analysis** on inconsistent cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXiAQpwcjtrZ"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytzPvwN0dPaw"
   },
   "source": [
    "# ðŸ› ï¸ 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31mÃ—\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31mâ•°â”€>\u001b[0m \u001b[31m[160 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m torch.__version__  = 2.9.1+cu128\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m /usr/lib/python3/dist-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m Guessing wheel URL:  https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.9cxx11abiTRUE-cp310-cp310-linux_x86_64.whl\n",
      "  \u001b[31m   \u001b[0m Precompiled wheel not found. Building from source...\n",
      "  \u001b[31m   \u001b[0m W1120 13:26:19.160000 232005 torch/utils/cpp_extension.py:630] Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/generate_kernels.py -> build/lib.linux-x86_64-3.10/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/benchmark_flash_attention_fp8.py -> build/lib.linux-x86_64-3.10/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/test_util.py -> build/lib.linux-x86_64-3.10/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/flash_attn_interface.py -> build/lib.linux-x86_64-3.10/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/test_kvcache.py -> build/lib.linux-x86_64-3.10/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/setup.py -> build/lib.linux-x86_64-3.10/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/benchmark_split_kv.py -> build/lib.linux-x86_64-3.10/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/__init__.py -> build/lib.linux-x86_64-3.10/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/test_flash_attn.py -> build/lib.linux-x86_64-3.10/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/test_attn_kvcache.py -> build/lib.linux-x86_64-3.10/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/padding.py -> build/lib.linux-x86_64-3.10/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/benchmark_mla_decode.py -> build/lib.linux-x86_64-3.10/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/benchmark_attn.py -> build/lib.linux-x86_64-3.10/hopper\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_blocksparse_attention.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_interface.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_og.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/bert_padding.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_blocksparse_attn_interface.py -> build/lib.linux-x86_64-3.10/flash_attn\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/baichuan.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/bigcode.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/vit.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/gpt.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/gpt_neox.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/opt.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/bert.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/llama.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/btlm.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/gptj.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/falcon.py -> build/lib.linux-x86_64-3.10/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/bwd_prefill_onekernel.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/bwd_prefill_fused.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/fwd_prefill.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/fwd_decode.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/bwd_prefill_split.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/test.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/interface_fa.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/bwd_ref.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/fwd_ref.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/fp8.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/train.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/utils.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/bwd_prefill.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/bench.py -> build/lib.linux-x86_64-3.10/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/activations.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/rms_norm.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/layer_norm.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/fused_dense.py -> build/lib.linux-x86_64-3.10/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/losses/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/losses/cross_entropy.py -> build/lib.linux-x86_64-3.10/flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/benchmark.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/distributed.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/generation.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/pretrained.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/testing.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/torch.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/library.py -> build/lib.linux-x86_64-3.10/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/flash_bwd_postprocess.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/hopper_helpers.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/tile_scheduler.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/fast_math.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/seqlen_info.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/flash_fwd_sm100.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/blackwell_helpers.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/mask.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/pack_gqa.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/flash_bwd.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/pipeline.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/ampere_helpers.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/softmax.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/utils.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/block_info.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/named_barrier.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/mma_sm100_desc.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/interface.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/flash_bwd_preprocess.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/cute/flash_fwd.py -> build/lib.linux-x86_64-3.10/flash_attn/cute\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/embedding.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/mha.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/mlp.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/block.py -> build/lib.linux-x86_64-3.10/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/layers/rotary.py -> build/lib.linux-x86_64-3.10/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/layers/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/layers/patch_embed.py -> build/lib.linux-x86_64-3.10/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/linear.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/rotary.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/__init__.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/k_activations.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/mlp.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/cross_entropy.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/layer_norm.py -> build/lib.linux-x86_64-3.10/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m W1120 13:26:19.167000 232005 torch/utils/cpp_extension.py:521] The detected CUDA version (12.6) has a minor version mismatch with the version that was used to compile PyTorch (12.8). Most likely this shouldn't be a problem.\n",
      "  \u001b[31m   \u001b[0m W1120 13:26:19.168000 232005 torch/utils/cpp_extension.py:531] There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.6\n",
      "  \u001b[31m   \u001b[0m building 'flash_attn_2_cuda' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.10\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.10/csrc\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.10/csrc/flash_attn\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.10/csrc/flash_attn/src\n",
      "  \u001b[31m   \u001b[0m x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/flash_api.cpp -o build/temp.linux-x86_64-3.10/csrc/flash_attn/flash_api.o -O3 -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim128_bf16_causal_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim128_bf16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim128_fp16_causal_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim128_fp16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim192_bf16_causal_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim192_bf16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim192_bf16_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim192_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim192_fp16_causal_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim192_fp16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim192_fp16_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim192_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim256_bf16_causal_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim256_bf16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim256_bf16_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim256_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim256_fp16_causal_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim256_fp16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim256_fp16_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim256_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim32_bf16_causal_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim32_bf16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim32_bf16_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim32_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim32_fp16_causal_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim32_fp16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim32_fp16_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim32_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim64_bf16_causal_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim64_bf16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim64_bf16_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim64_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim64_fp16_causal_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim64_fp16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim96_bf16_causal_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim96_bf16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim96_bf16_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim96_bf16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim96_fp16_causal_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim96_fp16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m /usr/local/cuda/bin/nvcc -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/flash_attn/src -I/tmp/pip-install-urfn8wrv/flash-attn_e97300da482f43a4bfef206abc517fa8/csrc/cutlass/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include -I/home/luisastellet/.local/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/usr/local/cuda/include -I/usr/include/python3.10 -c csrc/flash_attn/src/flash_fwd_hdim128_bf16_causal_sm80.cu -o build/temp.linux-x86_64-3.10/csrc/flash_attn/src/flash_fwd_hdim128_bf16_causal_sm80.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_80,code=sm_80 -gencode arch=compute_90,code=sm_90 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=flash_attn_2_cuda\n",
      "  \u001b[31m   \u001b[0m Segmentation fault (core dumped)\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/local/cuda/bin/nvcc' failed with exit code 255\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for flash-attn\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Para ver o progresso, remova o -q\n",
    "!pip install accelerate datasets peft bitsandbytes tensorboard pandas\n",
    "!pip install flash-attn --no-build-isolation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 111510,
     "status": "ok",
     "timestamp": 1763566097425,
     "user": {
      "displayName": "Luisa Muniz Stellet",
      "userId": "17907239366073679393"
     },
     "user_tz": 180
    },
    "id": "8dID70H5jtXH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, Idefics3ForConditionalGeneration, TrainingArguments, Trainer\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_LORA = False\n",
    "USE_QLORA = True\n",
    "\n",
    "model_id = \"HuggingFaceTB/SmolVLM-256M-Instruct\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eW75l-ZXg20e"
   },
   "source": [
    "# ðŸ—‚ï¸ 2. Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2880,
     "status": "ok",
     "timestamp": 1763566100391,
     "user": {
      "displayName": "Luisa Muniz Stellet",
      "userId": "17907239366073679393"
     },
     "user_tz": 180
    },
    "id": "5LxojVtgd0Vt",
    "outputId": "3ae0fdad-8dfb-4354-a2bd-c6cfc204853a"
   },
   "outputs": [],
   "source": [
    "# === Caminhos dos CSVs (ajuste se preferir) ===\n",
    "CSV_TRAIN = \"../train/dados_espanhol_balanceado.csv\"\n",
    "CSV_VAL   = \"../validation/dados_espanhol.csv\"\n",
    "CSV_TEST  = \"../test/dados_espanhol.csv\"\n",
    "# === Caminhos das pastas de imagens ===\n",
    "TRAIN_IMAGES_DIR = \"train_images\"\n",
    "VAL_IMAGES_DIR   = \"validation_images\"\n",
    "TEST_IMAGES_DIR  = \"test_images\"\n",
    "# Carregar CSVs\n",
    "df_train = pd.read_csv(CSV_TRAIN)\n",
    "df_val   = pd.read_csv(CSV_VAL)\n",
    "df_test  = pd.read_csv(CSV_TEST)\n",
    "\n",
    "print(\"CSV train:\", df_train.shape)\n",
    "print(\"CSV val:\", df_val.shape)\n",
    "print(\"CSV test:\", df_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA32vZVhmTPZ"
   },
   "source": [
    "# âš™ï¸ 3. Main Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1763566100474,
     "user": {
      "displayName": "Luisa Muniz Stellet",
      "userId": "17907239366073679393"
     },
     "user_tz": 180
    },
    "id": "rB2p5GBMmSy3",
    "outputId": "5df7b60f-3f08-4cbb-eddc-2df75e0de487"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Quantidade de exemplos por label:\n",
      "label\n",
      "hate speech              600\n",
      "inappropriate content    472\n",
      "neither                  386\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Model & Training Params ---\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 4\n",
    "LR = 2e-5\n",
    "OUTPUT_DIR = \"./SmolVLM_DIMEMEX_ImageOnly\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Label distribution (string labels only) ---\n",
    "print(\"\\nðŸ“Š Quantidade de exemplos por label:\")\n",
    "print(df_train[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yhRmaFJmgU9"
   },
   "source": [
    "# ðŸ“¦ 4. Prepare Train, Validation, and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 379,
     "status": "ok",
     "timestamp": 1763566100861,
     "user": {
      "displayName": "Luisa Muniz Stellet",
      "userId": "17907239366073679393"
     },
     "user_tz": 180
    },
    "id": "io9BUBZ9d0a9",
    "outputId": "473b004d-4cc9-45a6-f03a-6ec1729d7e68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1458 | Validation: 322 | Test: 648\n"
     ]
    }
   ],
   "source": [
    "# Os dados jÃ¡ estÃ£o divididos\n",
    "\n",
    "df_train[\"image_path\"] = df_train[\"image_path\"].apply(lambda x: os.path.join(TRAIN_IMAGES_DIR, x))\n",
    "df_val[\"image_path\"]   = df_val[\"image_path\"].apply(lambda x: os.path.join(VAL_IMAGES_DIR, x))\n",
    "df_test[\"image_path\"]  = df_test[\"image_path\"].apply(lambda x: os.path.join(TEST_IMAGES_DIR, x))\n",
    "\n",
    "ds_train = Dataset.from_pandas(df_train.reset_index(drop=True))\n",
    "ds_val = Dataset.from_pandas(df_val.reset_index(drop=True))\n",
    "ds_test = Dataset.from_pandas(df_test.reset_index(drop=True))\n",
    "\n",
    "print(f\"Train: {len(ds_train)} | Validation: {len(ds_val)} | Test: {len(ds_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHKY9-qRmluv"
   },
   "source": [
    "# ðŸ§© 5. Processor and Pre-processing **(Image)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Datasets recriados com sucesso!\n",
      "Colunas: ['images', 'messages', 'image_path']\n",
      "Exemplo de formataÃ§Ã£o:\n",
      "User: Clasifica este meme: hate speech, inappropriate content, o neither\n",
      "Assistant: neither\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "\n",
    "if USE_QLORA or USE_LORA:\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=['down_proj','o_proj','k_proj','q_proj','gate_proj','up_proj','v_proj'],\n",
    "        use_dora=False if USE_QLORA else True,\n",
    "        init_lora_weights=\"gaussian\"\n",
    "    )\n",
    "    lora_config.inference_mode = False\n",
    "    if USE_QLORA:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "    model = Idefics3ForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config if USE_QLORA else None,\n",
    "        _attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model.add_adapter(lora_config)\n",
    "    model.enable_adapters()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(model.get_nb_trainable_parameters())\n",
    "else:\n",
    "    model = Idefics3ForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        _attn_implementation=\"flash_attention_2\",\n",
    "    ).to(device)\n",
    "\n",
    "    for param in model.model.vision_model.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rcvCWr0mza-"
   },
   "source": [
    "# ðŸ§® 7. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_token_id = processor.tokenizer.additional_special_tokens_ids[\n",
    "            processor.tokenizer.additional_special_tokens.index(\"<image>\")]\n",
    "\n",
    "def format_with_original_label(example, label):\n",
    "    path = example[\"image_path\"]\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            image = Image.new(\"RGB\", (224, 224), \"black\")\n",
    "        else:\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "            image.thumbnail((512, 512))  \n",
    "    except Exception:\n",
    "        image = Image.new(\"RGB\", (224, 224), \"black\")\n",
    "\n",
    "    return {\"image\": image, \"label\": label, \"image_path\": path}\n",
    "\n",
    "# Criar datasets\n",
    "train_data = [\n",
    "    format_with_original_label({\"image_path\": row[\"image_path\"]}, row[\"label\"])\n",
    "    for _, row in df_train.iterrows()\n",
    "]\n",
    "\n",
    "val_data = [\n",
    "    format_with_original_label({\"image_path\": row[\"image_path\"]}, row[\"label\"])\n",
    "    for _, row in df_val.iterrows()\n",
    "]\n",
    "\n",
    "ds_train = Dataset.from_list(train_data)\n",
    "ds_val = Dataset.from_list(val_data)\n",
    "\n",
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "    for example in examples:\n",
    "        image = example[\"image\"]\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        label = example[\"label\"]\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": \"Clasifica este meme: hate speech, inappropriate content, o neither\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": label}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        text = processor.apply_chat_template(messages, add_generation_prompt=False)\n",
    "        texts.append(text.strip())\n",
    "        images.append([image])\n",
    "\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch\n",
    "\n",
    "data_collator = collate_fn\n",
    "# Instanciar o data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1763567309773,
     "user": {
      "displayName": "Luisa Muniz Stellet",
      "userId": "17907239366073679393"
     },
     "user_tz": 180
    },
    "id": "6cfDG7QniOO8"
   },
   "outputs": [],
   "source": [
    "# Configurar Argumentos de Treino\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=4,\n",
    "    ddp_find_unused_parameters=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_val,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWgB2mX8nH1V"
   },
   "source": [
    "# 9. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "c7MraJ2nd0m_",
    "outputId": "42892fd9-a013-4ec5-cad4-50262648f1e7"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/luisastellet/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/luisastellet/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/tmp/ipykernel_228903/1094426231.py\", line 16, in __call__\n    batch = self.processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n  File \"/tmp/ipykernel_228903/3661737659.py\", line 26, in __call__\n    text_inputs = self.tokenizer(\nTypeError: GPT2TokenizerFast(name_or_path='HuggingFaceTB/SmolVLM-256M-Instruct', vocab_size=49152, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<|im_start|>', 'eos_token': '<end_of_utterance>', 'unk_token': '<|endoftext|>', 'pad_token': '<|im_end|>', 'additional_special_tokens': ['<fake_token_around_image>', '<image>', '<end_of_utterance>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken(\"<repo_name>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t4: AddedToken(\"<reponame>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t5: AddedToken(\"<file_sep>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t6: AddedToken(\"<filename>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t7: AddedToken(\"<gh_stars>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t8: AddedToken(\"<issue_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t9: AddedToken(\"<issue_comment>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t10: AddedToken(\"<issue_closed>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t11: AddedToken(\"<jupyter_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t12: AddedToken(\"<jupyter_text>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t13: AddedToken(\"<jupyter_code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t14: AddedToken(\"<jupyter_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t15: AddedToken(\"<jupyter_script>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t16: AddedToken(\"<empty_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49152: AddedToken(\"<global-img>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49153: AddedToken(\"<row_1_col_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49154: AddedToken(\"<row_1_col_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49155: AddedToken(\"<row_1_col_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49156: AddedToken(\"<row_1_col_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49157: AddedToken(\"<row_1_col_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49158: AddedToken(\"<row_1_col_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49159: AddedToken(\"<row_2_col_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49160: AddedToken(\"<row_2_col_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49161: AddedToken(\"<row_2_col_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49162: AddedToken(\"<row_2_col_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49163: AddedToken(\"<row_2_col_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49164: AddedToken(\"<row_2_col_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49165: AddedToken(\"<row_3_col_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49166: AddedToken(\"<row_3_col_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49167: AddedToken(\"<row_3_col_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49168: AddedToken(\"<row_3_col_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49169: AddedToken(\"<row_3_col_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49170: AddedToken(\"<row_3_col_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49171: AddedToken(\"<row_4_col_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49172: AddedToken(\"<row_4_col_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49173: AddedToken(\"<row_4_col_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49174: AddedToken(\"<row_4_col_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49175: AddedToken(\"<row_4_col_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49176: AddedToken(\"<row_4_col_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49177: AddedToken(\"<row_5_col_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49178: AddedToken(\"<row_5_col_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49179: AddedToken(\"<row_5_col_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49180: AddedToken(\"<row_5_col_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49181: AddedToken(\"<row_5_col_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49182: AddedToken(\"<row_5_col_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49183: AddedToken(\"<row_6_col_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49184: AddedToken(\"<row_6_col_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49185: AddedToken(\"<row_6_col_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49186: AddedToken(\"<row_6_col_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49187: AddedToken(\"<row_6_col_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49188: AddedToken(\"<row_6_col_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49189: AddedToken(\"<fake_token_around_image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49190: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49191: AddedToken(\"<|reserved_special_token_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49192: AddedToken(\"<|reserved_special_token_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49193: AddedToken(\"<|reserved_special_token_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49194: AddedToken(\"<|reserved_special_token_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49195: AddedToken(\"<|reserved_special_token_4|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49196: AddedToken(\"<|reserved_special_token_5|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49197: AddedToken(\"<|reserved_special_token_6|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49198: AddedToken(\"<|reserved_special_token_7|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49199: AddedToken(\"<|reserved_special_token_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49200: AddedToken(\"<|reserved_special_token_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49201: AddedToken(\"<|reserved_special_token_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49202: AddedToken(\"<|reserved_special_token_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49203: AddedToken(\"<|reserved_special_token_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49204: AddedToken(\"<|reserved_special_token_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49205: AddedToken(\"<|reserved_special_token_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49206: AddedToken(\"<|reserved_special_token_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49207: AddedToken(\"<|reserved_special_token_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49208: AddedToken(\"<|reserved_special_token_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49209: AddedToken(\"<|reserved_special_token_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49210: AddedToken(\"<|reserved_special_token_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49211: AddedToken(\"<|reserved_special_token_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49212: AddedToken(\"<|reserved_special_token_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49213: AddedToken(\"<|reserved_special_token_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49214: AddedToken(\"<|reserved_special_token_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49215: AddedToken(\"<|reserved_special_token_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49216: AddedToken(\"<|reserved_special_token_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49217: AddedToken(\"<|reserved_special_token_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49218: AddedToken(\"<|reserved_special_token_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49219: AddedToken(\"<|reserved_special_token_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49220: AddedToken(\"<|reserved_special_token_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49221: AddedToken(\"<|reserved_special_token_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49222: AddedToken(\"<|reserved_special_token_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49223: AddedToken(\"<|reserved_special_token_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49224: AddedToken(\"<|reserved_special_token_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49225: AddedToken(\"<|reserved_special_token_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49226: AddedToken(\"<|reserved_special_token_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49227: AddedToken(\"<|reserved_special_token_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49228: AddedToken(\"<|reserved_special_token_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49229: AddedToken(\"<|reserved_special_token_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49230: AddedToken(\"<|reserved_special_token_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49231: AddedToken(\"<|reserved_special_token_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49232: AddedToken(\"<|reserved_special_token_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49233: AddedToken(\"<|reserved_special_token_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49234: AddedToken(\"<|reserved_special_token_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49235: AddedToken(\"<|reserved_special_token_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49236: AddedToken(\"<|reserved_special_token_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49237: AddedToken(\"<|reserved_special_token_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49238: AddedToken(\"<|reserved_special_token_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49239: AddedToken(\"<|reserved_special_token_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49240: AddedToken(\"<|reserved_special_token_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49241: AddedToken(\"<|reserved_special_token_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49242: AddedToken(\"<|reserved_special_token_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49243: AddedToken(\"<|reserved_special_token_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49244: AddedToken(\"<|reserved_special_token_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49245: AddedToken(\"<|reserved_special_token_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49246: AddedToken(\"<|reserved_special_token_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49247: AddedToken(\"<|reserved_special_token_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49248: AddedToken(\"<|reserved_special_token_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49249: AddedToken(\"<|reserved_special_token_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49250: AddedToken(\"<|reserved_special_token_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49251: AddedToken(\"<|reserved_special_token_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49252: AddedToken(\"<|reserved_special_token_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49253: AddedToken(\"<|reserved_special_token_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49254: AddedToken(\"<|reserved_special_token_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49255: AddedToken(\"<|reserved_special_token_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49256: AddedToken(\"<|reserved_special_token_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49257: AddedToken(\"<|reserved_special_token_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49258: AddedToken(\"<|reserved_special_token_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49259: AddedToken(\"<|reserved_special_token_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49260: AddedToken(\"<|reserved_special_token_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49261: AddedToken(\"<|reserved_special_token_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49262: AddedToken(\"<|reserved_special_token_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49263: AddedToken(\"<|reserved_special_token_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49264: AddedToken(\"<|reserved_special_token_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49265: AddedToken(\"<|reserved_special_token_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49266: AddedToken(\"<|reserved_special_token_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49267: AddedToken(\"<|reserved_special_token_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49268: AddedToken(\"<|reserved_special_token_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49269: AddedToken(\"<|reserved_special_token_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49270: AddedToken(\"<|reserved_special_token_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49271: AddedToken(\"<|reserved_special_token_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49272: AddedToken(\"<|reserved_special_token_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49273: AddedToken(\"<|reserved_special_token_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49274: AddedToken(\"<|reserved_special_token_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49275: AddedToken(\"<|reserved_special_token_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49276: AddedToken(\"<|reserved_special_token_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49277: AddedToken(\"<|reserved_special_token_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49278: AddedToken(\"<|reserved_special_token_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49279: AddedToken(\"<end_of_utterance>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n} got multiple values for keyword argument 'padding'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine-tuning complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2122\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2120\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2123\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2426\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2424\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2425\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[0;32m-> 2426\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs \u001b[38;5;129;01min\u001b[39;00m batch_samples:\n\u001b[1;32m   2428\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:5038\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[1;32m   5036\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m   5037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5038\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m   5039\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   5040\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/data_loader.py:567\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/luisastellet/.local/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/luisastellet/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/tmp/ipykernel_228903/1094426231.py\", line 16, in __call__\n    batch = self.processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n  File \"/tmp/ipykernel_228903/3661737659.py\", line 26, in __call__\n    text_inputs = self.tokenizer(\nTypeError: GPT2TokenizerFast(name_or_path='HuggingFaceTB/SmolVLM-256M-Instruct', vocab_size=49152, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '<|im_start|>', 'eos_token': '<end_of_utterance>', 'unk_token': '<|endoftext|>', 'pad_token': '<|im_end|>', 'additional_special_tokens': ['<fake_token_around_image>', '<image>', '<end_of_utterance>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t3: AddedToken(\"<repo_name>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t4: AddedToken(\"<reponame>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t5: AddedToken(\"<file_sep>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t6: AddedToken(\"<filename>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t7: AddedToken(\"<gh_stars>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t8: AddedToken(\"<issue_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t9: AddedToken(\"<issue_comment>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t10: AddedToken(\"<issue_closed>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t11: AddedToken(\"<jupyter_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t12: AddedToken(\"<jupyter_text>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t13: AddedToken(\"<jupyter_code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t14: AddedToken(\"<jupyter_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t15: AddedToken(\"<jupyter_script>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t16: AddedToken(\"<empty_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49152: AddedToken(\"<global-img>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49153: AddedToken(\"<row_1_col_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49154: AddedToken(\"<row_1_col_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49155: AddedToken(\"<row_1_col_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49156: AddedToken(\"<row_1_col_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49157: AddedToken(\"<row_1_col_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49158: AddedToken(\"<row_1_col_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49159: AddedToken(\"<row_2_col_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49160: AddedToken(\"<row_2_col_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49161: AddedToken(\"<row_2_col_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49162: AddedToken(\"<row_2_col_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49163: AddedToken(\"<row_2_col_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49164: AddedToken(\"<row_2_col_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49165: AddedToken(\"<row_3_col_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49166: AddedToken(\"<row_3_col_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49167: AddedToken(\"<row_3_col_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49168: AddedToken(\"<row_3_col_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49169: AddedToken(\"<row_3_col_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49170: AddedToken(\"<row_3_col_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49171: AddedToken(\"<row_4_col_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49172: AddedToken(\"<row_4_col_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49173: AddedToken(\"<row_4_col_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49174: AddedToken(\"<row_4_col_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49175: AddedToken(\"<row_4_col_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49176: AddedToken(\"<row_4_col_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49177: AddedToken(\"<row_5_col_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49178: AddedToken(\"<row_5_col_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49179: AddedToken(\"<row_5_col_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49180: AddedToken(\"<row_5_col_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49181: AddedToken(\"<row_5_col_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49182: AddedToken(\"<row_5_col_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49183: AddedToken(\"<row_6_col_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49184: AddedToken(\"<row_6_col_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49185: AddedToken(\"<row_6_col_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49186: AddedToken(\"<row_6_col_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49187: AddedToken(\"<row_6_col_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49188: AddedToken(\"<row_6_col_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49189: AddedToken(\"<fake_token_around_image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49190: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49191: AddedToken(\"<|reserved_special_token_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49192: AddedToken(\"<|reserved_special_token_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49193: AddedToken(\"<|reserved_special_token_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49194: AddedToken(\"<|reserved_special_token_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49195: AddedToken(\"<|reserved_special_token_4|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49196: AddedToken(\"<|reserved_special_token_5|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49197: AddedToken(\"<|reserved_special_token_6|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49198: AddedToken(\"<|reserved_special_token_7|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49199: AddedToken(\"<|reserved_special_token_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49200: AddedToken(\"<|reserved_special_token_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49201: AddedToken(\"<|reserved_special_token_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49202: AddedToken(\"<|reserved_special_token_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49203: AddedToken(\"<|reserved_special_token_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49204: AddedToken(\"<|reserved_special_token_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49205: AddedToken(\"<|reserved_special_token_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49206: AddedToken(\"<|reserved_special_token_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49207: AddedToken(\"<|reserved_special_token_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49208: AddedToken(\"<|reserved_special_token_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49209: AddedToken(\"<|reserved_special_token_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49210: AddedToken(\"<|reserved_special_token_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49211: AddedToken(\"<|reserved_special_token_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49212: AddedToken(\"<|reserved_special_token_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49213: AddedToken(\"<|reserved_special_token_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49214: AddedToken(\"<|reserved_special_token_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49215: AddedToken(\"<|reserved_special_token_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49216: AddedToken(\"<|reserved_special_token_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49217: AddedToken(\"<|reserved_special_token_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49218: AddedToken(\"<|reserved_special_token_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49219: AddedToken(\"<|reserved_special_token_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49220: AddedToken(\"<|reserved_special_token_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49221: AddedToken(\"<|reserved_special_token_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49222: AddedToken(\"<|reserved_special_token_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49223: AddedToken(\"<|reserved_special_token_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49224: AddedToken(\"<|reserved_special_token_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49225: AddedToken(\"<|reserved_special_token_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49226: AddedToken(\"<|reserved_special_token_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49227: AddedToken(\"<|reserved_special_token_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49228: AddedToken(\"<|reserved_special_token_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49229: AddedToken(\"<|reserved_special_token_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49230: AddedToken(\"<|reserved_special_token_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49231: AddedToken(\"<|reserved_special_token_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49232: AddedToken(\"<|reserved_special_token_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49233: AddedToken(\"<|reserved_special_token_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49234: AddedToken(\"<|reserved_special_token_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49235: AddedToken(\"<|reserved_special_token_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49236: AddedToken(\"<|reserved_special_token_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49237: AddedToken(\"<|reserved_special_token_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49238: AddedToken(\"<|reserved_special_token_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49239: AddedToken(\"<|reserved_special_token_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49240: AddedToken(\"<|reserved_special_token_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49241: AddedToken(\"<|reserved_special_token_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49242: AddedToken(\"<|reserved_special_token_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49243: AddedToken(\"<|reserved_special_token_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49244: AddedToken(\"<|reserved_special_token_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49245: AddedToken(\"<|reserved_special_token_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49246: AddedToken(\"<|reserved_special_token_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49247: AddedToken(\"<|reserved_special_token_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49248: AddedToken(\"<|reserved_special_token_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49249: AddedToken(\"<|reserved_special_token_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49250: AddedToken(\"<|reserved_special_token_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49251: AddedToken(\"<|reserved_special_token_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49252: AddedToken(\"<|reserved_special_token_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49253: AddedToken(\"<|reserved_special_token_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49254: AddedToken(\"<|reserved_special_token_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49255: AddedToken(\"<|reserved_special_token_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49256: AddedToken(\"<|reserved_special_token_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49257: AddedToken(\"<|reserved_special_token_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49258: AddedToken(\"<|reserved_special_token_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49259: AddedToken(\"<|reserved_special_token_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49260: AddedToken(\"<|reserved_special_token_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49261: AddedToken(\"<|reserved_special_token_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49262: AddedToken(\"<|reserved_special_token_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49263: AddedToken(\"<|reserved_special_token_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49264: AddedToken(\"<|reserved_special_token_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49265: AddedToken(\"<|reserved_special_token_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49266: AddedToken(\"<|reserved_special_token_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49267: AddedToken(\"<|reserved_special_token_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49268: AddedToken(\"<|reserved_special_token_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49269: AddedToken(\"<|reserved_special_token_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49270: AddedToken(\"<|reserved_special_token_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49271: AddedToken(\"<|reserved_special_token_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49272: AddedToken(\"<|reserved_special_token_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49273: AddedToken(\"<|reserved_special_token_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49274: AddedToken(\"<|reserved_special_token_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49275: AddedToken(\"<|reserved_special_token_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49276: AddedToken(\"<|reserved_special_token_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49277: AddedToken(\"<|reserved_special_token_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49278: AddedToken(\"<|reserved_special_token_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t49279: AddedToken(\"<end_of_utterance>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n} got multiple values for keyword argument 'padding'\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "print(\"Fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjgMBwjcnRIT"
   },
   "source": [
    "# 10. Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 132492,
     "status": "aborted",
     "timestamp": 1763566101005,
     "user": {
      "displayName": "Luisa Muniz Stellet",
      "userId": "17907239366073679393"
     },
     "user_tz": 180
    },
    "id": "nBWwMBgBd0p3"
   },
   "outputs": [],
   "source": [
    "# Garante que o diretÃ³rio de saÃ­da existe\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# Acessa o histÃ³rico direto da memÃ³ria\n",
    "logs = trainer.state.log_history\n",
    "\n",
    "train_steps = [x[\"step\"] for x in logs if \"loss\" in x]\n",
    "train_loss = [x[\"loss\"] for x in logs if \"loss\" in x]\n",
    "\n",
    "eval_steps = [x[\"step\"] for x in logs if \"eval_loss\" in x]\n",
    "eval_loss = [x[\"eval_loss\"] for x in logs if \"eval_loss\" in x]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_steps, train_loss, label=\"Train Loss\")\n",
    "plt.plot(eval_steps, eval_loss, label=\"Eval Loss\", marker='o')\n",
    "plt.title(\"Training and Evaluation Loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# --- SALVAR O GRÃFICO ---\n",
    "plot_path = os.path.join(OUTPUT_DIR, \"loss_curve.png\")\n",
    "plt.savefig(plot_path)\n",
    "print(f\"ðŸ“‰ GrÃ¡fico de Loss salvo em: {plot_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94ruhuHv2r8g"
   },
   "source": [
    "### **SÃ³ pode rodar a cÃ©lula abaixo quando estiver satisfeito com os resultado! ParÃ£o nÃ£o ter vazamento de dados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pLr8FmZnOlE"
   },
   "source": [
    "# âœ… 11. Final Evaluation and Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 132492,
     "status": "aborted",
     "timestamp": 1763566101007,
     "user": {
      "displayName": "Luisa Muniz Stellet",
      "userId": "17907239366073679393"
     },
     "user_tz": 180
    },
    "id": "KlaHEt12nO9f"
   },
   "outputs": [],
   "source": [
    "# Use o modelo treinado\n",
    "model = trainer.model\n",
    "model.eval()\n",
    "\n",
    "test_predictions = []\n",
    "test_ground_truth = []\n",
    "valid_labels = [\"hate speech\", \"inappropriate content\", \"neither\"]\n",
    "\n",
    "print(\"ðŸš€ Rodando avaliaÃ§Ã£o final no conjunto de teste...\")\n",
    "\n",
    "# Loop pelo Test Set\n",
    "for example in tqdm(ds_test):\n",
    "    try:\n",
    "        image = Image.open(example['image_path']).convert(\"RGB\")\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    prompt_text = \"\"\"\"\n",
    "                Analiza esta imagen de meme y clasifÃ­cala en una de estas tres categorÃ­as: \n",
    "                - hate speech\n",
    "                - inappropriate content\n",
    "                - neither\n",
    "                Â¿CuÃ¡l es la categorÃ­a correcta para este meme?\n",
    "                \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": prompt_text}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n",
    "\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Limpeza da resposta\n",
    "    if \"Assistant:\" in generated_text:\n",
    "        prediction = generated_text.split(\"Assistant:\")[-1].strip()\n",
    "    elif \"assistant\" in generated_text:\n",
    "         prediction = generated_text.split(\"assistant\")[-1].strip()\n",
    "    else:\n",
    "        prediction = generated_text.strip()\n",
    "\n",
    "    test_predictions.append(prediction)\n",
    "    test_ground_truth.append(example['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zav_mgz_3nO8"
   },
   "outputs": [],
   "source": [
    "# --- Processamento e Salvamento dos Resultados ---\n",
    "\n",
    "# 1. Limpeza e Filtro\n",
    "cleaned_preds = []\n",
    "for p in test_predictions:\n",
    "    p_clean = p.lower().replace(\".\", \"\").strip()\n",
    "    if p_clean in valid_labels:\n",
    "        cleaned_preds.append(p_clean)\n",
    "    else:\n",
    "        cleaned_preds.append(\"unknown\") # Marca erro de geraÃ§Ã£o\n",
    "\n",
    "# 2. Preparar listas finais\n",
    "final_preds = []\n",
    "final_gt = []\n",
    "for p, g in zip(cleaned_preds, test_ground_truth):\n",
    "    if p in valid_labels:\n",
    "        final_preds.append(p)\n",
    "        final_gt.append(g)\n",
    "\n",
    "# 3. SALVAR PREDIÃ‡Ã•ES (CSV)\n",
    "# Isso cria uma tabela com: Label Real | PrediÃ§Ã£o | Acertou?\n",
    "df_results = pd.DataFrame({\n",
    "    \"ground_truth\": final_gt,\n",
    "    \"prediction\": final_preds\n",
    "})\n",
    "df_results[\"correct\"] = df_results[\"ground_truth\"] == df_results[\"prediction\"]\n",
    "csv_path = os.path.join(OUTPUT_DIR, \"test_predictions.csv\")\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(f\"\\nðŸ’¾ Tabela de prediÃ§Ãµes salva em: {csv_path}\")\n",
    "\n",
    "# 4. Calcular MÃ©tricas\n",
    "report_str = classification_report(final_gt, final_preds, labels=valid_labels)\n",
    "results = {\n",
    "    \"accuracy\": accuracy_score(final_gt, final_preds),\n",
    "    \"f1_weighted\": f1_score(final_gt, final_preds, average=\"weighted\"),\n",
    "    \"precision_weighted\": precision_score(final_gt, final_preds, average=\"weighted\", zero_division=0),\n",
    "    \"recall_weighted\": recall_score(final_gt, final_preds, average=\"weighted\", zero_division=0)\n",
    "}\n",
    "\n",
    "# 5. SALVAR RELATÃ“RIO (TXT e JSON)\n",
    "txt_path = os.path.join(OUTPUT_DIR, \"classification_report.txt\")\n",
    "with open(txt_path, \"w\") as f:\n",
    "    f.write(report_str)\n",
    "    f.write(\"\\n\\nSummary Metrics:\\n\")\n",
    "    f.write(json.dumps(results, indent=4))\n",
    "\n",
    "json_path = os.path.join(OUTPUT_DIR, \"metrics.json\")\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(f\"ðŸ“„ RelatÃ³rios salvos em: {txt_path} e {json_path}\")\n",
    "\n",
    "# Exibir na tela\n",
    "print(f\"\\n{len(final_preds)} / {len(test_predictions)} prediÃ§Ãµes vÃ¡lidas.\")\n",
    "print(\"\\nðŸ“Š Resultados Finais:\")\n",
    "print(report_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KwfLD7CnPCV"
   },
   "source": [
    "# ðŸ’¾ 12. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 132491,
     "status": "aborted",
     "timestamp": 1763566101009,
     "user": {
      "displayName": "Luisa Muniz Stellet",
      "userId": "17907239366073679393"
     },
     "user_tz": 180
    },
    "id": "RV699iGtnPNf"
   },
   "outputs": [],
   "source": [
    "# Save the LoRA adapters\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "# Save the processor\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"\\nâœ… Fine-tuning complete. Model saved to: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06768d1fabde49beb8307f5956953e81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0794220023644beb8926208da8d92bb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "09cf3776db4549cbaa6e13bd77ef6a35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a4dc9b53ff74645a9ab72fd4d079d60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d9b4a8f785a4841b2e8e56737f1dccb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23d75496e01e4dc8bba86f7f83796aa9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8b68974fdeb647a18c786f0629ce12ee",
       "IPY_MODEL_c0afb1f7cf3045688be0251dcaab4d44",
       "IPY_MODEL_dc4fd90c53b74c52adce19f85659906b"
      ],
      "layout": "IPY_MODEL_31d45a4cea164555ba7555efe1042baf"
     }
    },
    "30c6ab2a209f494ba43ce49f241d02a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31d45a4cea164555ba7555efe1042baf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "405973e97f3c44109f2d0ef3afd8a8bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "447dc8fd5d27467ab89d8bb76dd905d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb5ad73985e6470a9b4f4c86c412efbe",
      "max": 2262,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dd2dbacdbf754b3c8c59ca7fc90684ab",
      "value": 2262
     }
    },
    "5f49d2482b2b446baef0417b60faaf72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b68974fdeb647a18c786f0629ce12ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d9b4a8f785a4841b2e8e56737f1dccb",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5f49d2482b2b446baef0417b60faaf72",
      "value": "Map:â€‡100%"
     }
    },
    "91aa195338a54a819c18ae049e756fad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09cf3776db4549cbaa6e13bd77ef6a35",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0794220023644beb8926208da8d92bb0",
      "value": "Map:â€‡100%"
     }
    },
    "c0afb1f7cf3045688be0251dcaab4d44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_405973e97f3c44109f2d0ef3afd8a8bb",
      "max": 322,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_06768d1fabde49beb8307f5956953e81",
      "value": 322
     }
    },
    "c40651952c8c49b7a5da9dd2bfdc056a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8b8c5f630af4a8f9b4cb87cb08f7f0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc4fd90c53b74c52adce19f85659906b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8b8c5f630af4a8f9b4cb87cb08f7f0b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e042c1e7043f4d9388570f6286334712",
      "value": "â€‡322/322â€‡[00:46&lt;00:00,â€‡â€‡1.00s/â€‡examples]"
     }
    },
    "dd2dbacdbf754b3c8c59ca7fc90684ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e042c1e7043f4d9388570f6286334712": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e5aa64a303de4cc9ab30a0976ffb236b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_91aa195338a54a819c18ae049e756fad",
       "IPY_MODEL_447dc8fd5d27467ab89d8bb76dd905d3",
       "IPY_MODEL_f328c9a53c53460da7b831f84b51a230"
      ],
      "layout": "IPY_MODEL_1a4dc9b53ff74645a9ab72fd4d079d60"
     }
    },
    "eb5ad73985e6470a9b4f4c86c412efbe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f328c9a53c53460da7b831f84b51a230": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30c6ab2a209f494ba43ce49f241d02a1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c40651952c8c49b7a5da9dd2bfdc056a",
      "value": "â€‡2262/2262â€‡[09:53&lt;00:00,â€‡â€‡1.38s/â€‡examples]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
