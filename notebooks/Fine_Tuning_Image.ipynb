{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **DIMEMEX ‚Äî Complete Project Pipeline Notebook**\n","\n","*Multilingual Meme Translation & Hate Speech Analysis*\n","\n","---\n","\n","## **Members**\n","\n","* **B√°rbara** (Text)\n","* **Amanda** (Text + Description)\n","* **Juan David Nieto** (Text + Description + Image)\n","* **Luisa** (Image)\n","\n","---\n","\n","## **Project Overview**\n","\n","This project analyzes whether **offensive or hate speech content in memes is preserved after translating the memes from Spanish to Portuguese**.\n","\n","We work with the **DIMEMEX** dataset, which contains:\n","\n","* Meme **text**\n","* Meme **description**\n","* Meme **image**\n","* Labels: *hate speech*, *inappropriate content*, *neither*\n","\n","This project has **two main tasks**:\n","\n","### **Task 1 ‚Äî Translation Quality Evaluation**\n","\n","Translate the Spanish text to Portuguese and evaluate translation quality using standard NLP metrics:\n","\n","* **BLEURT**\n","* **BERTScore**\n","* **COMET-Kiwi**\n","* **chrF**\n","\n","### **Task 2 ‚Äî Hate Speech Detection (Multimodal Fine-Tuning)**\n","\n","Fine-tune models to classify:\n","\n","* Hate speech\n","* Inappropriate content\n","* Neither\n","\n","We fine-tune under four input settings:\n","\n","1. Text\n","2. Text + Description\n","3. Text + Description + Image (Multimodal)\n","4. Image\n","\n","---\n","\n","## **üìå Objectives**\n","\n","1. Evaluate whether offensive content is **maintained or lost** during translation.\n","2. Compare performance of **original Spanish memes vs. translated Portuguese memes**.\n","3. Train and evaluate multimodal detectors to classify hate speech.\n","4. Analyze cases where the label changes across languages.\n","5. Perform **human qualitative analysis** on inconsistent cases.\n","\n"],"metadata":{"id":"QYlPUuApdPvc"}},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"jXiAQpwcjtrZ"}},{"cell_type":"markdown","source":["# üõ†Ô∏è 1. Install Dependencies"],"metadata":{"id":"ytzPvwN0dPaw"}},{"cell_type":"code","source":["!pip install -q transformers accelerate datasets peft bitsandbytes torch torchvision pillow tqdm scikit-learn matplotlib evaluate\n","!pip install -q trl # Install TRL for SFTTrainer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"heqyuCzFdOjk","outputId":"ecb11cd3-b4be-491a-aa58-203894f36fc0","executionInfo":{"status":"ok","timestamp":1763554971311,"user_tz":180,"elapsed":26755,"user":{"displayName":"Luisa Muniz Stellet","userId":"17907239366073679393"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m465.5/465.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import os\n","import zipfile\n","import json\n","import warnings\n","\n","import pandas as pd\n","import numpy as np\n","from PIL import Image\n","\n","from google.colab import files\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    accuracy_score,\n","    f1_score,\n","    precision_score,\n","    recall_score,\n","    classification_report\n",")\n","\n","import torch\n","\n","from datasets import Dataset\n","import evaluate\n","from evaluate import load\n","\n","# Hugging Face - Transformers (Models, Processors, Configs)\n","from transformers import (\n","    AutoTokenizer,\n","    AutoProcessor,\n","    AutoModelForCausalLM,\n","    Idefics3ForConditionalGeneration,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    pipeline\n",")\n","\n","# Hugging Face - PEFT (LoRA)\n","from peft import (\n","    LoraConfig,\n","    get_peft_model,\n","    prepare_model_for_kbit_training,\n","    PeftModel\n",")\n","\n","# Hugging Face - TRL (Training)\n","from trl import SFTTrainer\n","\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"8dID70H5jtXH","executionInfo":{"status":"ok","timestamp":1763555055809,"user_tz":180,"elapsed":84492,"user":{"displayName":"Luisa Muniz Stellet","userId":"17907239366073679393"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# üóÇÔ∏è 2. Upload Data"],"metadata":{"id":"eW75l-ZXg20e"}},{"cell_type":"code","source":["from google.colab import files\n","import zipfile, os\n","\n","print(\"üìÅ Upload your CSV file...\")\n","print(\"   (Must contain columns: 'image_path', 'label', 'text', 'description')\")\n","\n","try:\n","    uploaded = files.upload()\n","    csv_name = list(uploaded.keys())[0]\n","except Exception as e:\n","    print(f\"Error or interruption during CSV upload: {e}\")\n","    csv_name = None\n","\n","print(\"\\nüì¶ Now upload the ZIP file with all images (e.g., DIMEMEX_images.zip)...\")\n","try:\n","    uploaded2 = files.upload()\n","    zip_name = list(uploaded2.keys())[0]\n","except Exception as e:\n","    print(f\"Error or interruption during ZIP upload: {e}\")\n","    zip_name = None\n","\n","# Unzip images\n","IMAGE_ROOT = \"DIMEMEX_images\"\n","if zip_name:\n","    os.makedirs(IMAGE_ROOT, exist_ok=True)\n","    with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n","        zip_ref.extractall(IMAGE_ROOT)\n","    print(\"‚úÖ Files uploaded and extracted.\")\n","else:\n","    print(\"‚ö†Ô∏è ZIP file upload skipped or failed.\")"],"metadata":{"id":"5LxojVtgd0Vt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚öôÔ∏è 3. Dataset Configuration and Analysis"],"metadata":{"id":"t-pisk1lhoca"}},{"cell_type":"code","source":["import pandas as pd\n","from PIL import Image\n","\n","if csv_name:\n","    DATA_CSV = csv_name\n","    df = pd.read_csv(DATA_CSV)\n","\n","    # Verify required columns for multimodal input\n","    required_cols = ['image_path', 'label', 'text', 'description']\n","    if not all(col in df.columns for col in required_cols):\n","        print(f\"‚ùå ERROR: CSV must contain {required_cols}\")\n","    else:\n","        print(\"‚úÖ CSV loaded successfully.\")\n","\n","        # Update image paths to point to the extracted folder\n","        df[\"image_path\"] = df[\"image_path\"].apply(\n","            lambda p: os.path.join(IMAGE_ROOT, os.path.basename(p))\n","        )\n","\n","        # Filter out rows with missing images\n","        df = df[df[\"image_path\"].apply(os.path.exists)].reset_index(drop=True)\n","\n","        print(\"\\nüìä Total dataset size (with valid images):\", len(df))\n","        print(df[\"label\"].value_counts().rename_axis(\"Label\").reset_index(name=\"Count\"))\n","else:\n","    print(\"‚ùå No CSV file loaded. Cannot proceed.\")"],"metadata":{"id":"iTek5ySld0YS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚öôÔ∏è 4. Main Configurations"],"metadata":{"id":"EA32vZVhmTPZ"}},{"cell_type":"code","source":["# --- Model & Training Params ---\n","MODEL_ID = \"HuggingFaceTB/SmolVLM-256M-Instruct\"\n","BATCH_SIZE = 2\n","EPOCHS = 2\n","LR = 2e-4\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# --- Label Mapping ---\n","# We need this for the final evaluation\n","labels = df['label'].unique()\n","label_to_id = {label: i for i, label in enumerate(labels)}\n","id_to_label = {i: label for i, label in enumerate(labels)}\n","print(f\"Labels mapped: {label_to_id}\")"],"metadata":{"id":"rB2p5GBMmSy3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#üì¶ 5. Prepare Train, Validation, and Test Datasets"],"metadata":{"id":"1yhRmaFJmgU9"}},{"cell_type":"code","source":["# We use the 'label' column to stratify the splits\n","train_df, test_df = train_test_split(\n","    df, test_size=0.15, stratify=df[\"label\"], random_state=42\n",")\n","train_df, val_df = train_test_split(\n","    train_df, test_size=0.1, stratify=train_df[\"label\"], random_state=42\n",")\n","\n","ds_train = Dataset.from_pandas(train_df.reset_index(drop=True))\n","ds_val = Dataset.from_pandas(val_df.reset_index(drop=True))\n","ds_test = Dataset.from_pandas(test_df.reset_index(drop=True))\n","\n","print(f\"Train: {len(ds_train)} | Validation: {len(ds_val)} | Test: {len(ds_test)}\")"],"metadata":{"id":"io9BUBZ9d0a9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#üß© 6. Processor and Pre-processing **(Text + Desc + Image)**"],"metadata":{"id":"MHKY9-qRmluv"}},{"cell_type":"code","source":["processor = AutoProcessor.from_pretrained(MODEL_ID)\n","\n","def format_for_sft(example):\n","    \"\"\"\n","    Formats the example for the SmolVLM's Instruction (SFT) format.\n","    Question: 'Analyze the image, text, and description. What is the category?'\n","    Answer (Label): 'hate speech' / 'inappropriate content' / 'neither'\n","    \"\"\"\n","    # Load image\n","    try:\n","        image = Image.open(example['image_path']).convert(\"RGB\")\n","    except Exception:\n","        image = Image.new('RGB', (100, 100), color = 'gray') # Placeholder\n","\n","    # Construct the Multimodal Prompt\n","    prompt_text = (\n","        f\"Analyze the following meme components and classify it. \"\n","        f\"Text: {example['text']}\\n\"\n","        f\"Description: {example['description']}\\n\\n\"\n","        f\"What is the category? \"\n","    )\n","\n","    # SFT format: user message (image + text) and assistant response (label)\n","    messages = [\n","        {\"role\": \"user\", \"content\": [image, prompt_text]},\n","        {\"role\": \"assistant\", \"content\": example['label']}\n","    ]\n","\n","    # Use the processor to apply the chat template\n","    example['text'] = processor.apply_chat_template(\n","        messages, tokenize=False, add_generation_prompt=False\n","    )\n","    return example\n","\n","# Apply the formatting\n","# This creates a single 'text' column that SFTTrainer will use\n","ds_train = ds_train.map(format_for_sft, remove_columns=ds_train.column_names)\n","ds_val = ds_val.map(format_for_sft, remove_columns=ds_val.column_names)\n","# We apply this to the test set later, before evaluation"],"metadata":{"id":"clsba7u8mlXc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üß† 7. Model + LoRA"],"metadata":{"id":"59jSQJWImwiu"}},{"cell_type":"code","source":["# 4-bit Quantization\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","base_model = Idefics3ForConditionalGeneration.from_pretrained(\n","    MODEL_ID,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\",\n","    torch_dtype=torch.bfloat16,\n",")\n","\n","base_model = prepare_model_for_kbit_training(base_model)\n","\n","lora_cfg = LoraConfig(\n","    r=16,\n","    lora_alpha=32,\n","    lora_dropout=0.1,\n","    # Target modules updated for Idefics3/SmolVLM\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","model = get_peft_model(base_model, lora_cfg)\n","model.print_trainable_parameters()"],"metadata":{"id":"gkCSEECVd0h8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üßÆ 8. Training Configuration"],"metadata":{"id":"6rcvCWr0mza-"}},{"cell_type":"code","source":["OUTPUT_DIR = \"./SmolVLM_256M_Instruct_DIMEMEX_lora\"\n","\n","training_args = TrainingArguments(\n","    output_dir=OUTPUT_DIR,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    num_train_epochs=EPOCHS,\n","    learning_rate=LR,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    fp16=False, # Must be False for 4-bit\n","    bf16=True,  # Use bf16 for 4-bit\n","    load_best_model_at_end=True,\n","    logging_steps=50,\n","    save_total_limit=3,\n","    report_to=\"none\", # Disable wandb/tensorboard logging\n",")\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=ds_train,\n","    eval_dataset=ds_val,\n","    tokenizer=processor, # The processor handles tokenization\n","    dataset_text_field=\"text\", # The column we created in Section 6\n","    max_seq_length=1024,\n",")"],"metadata":{"id":"_IPy0_nEd0kd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 10. Train Model"],"metadata":{"id":"OWgB2mX8nH1V"}},{"cell_type":"code","source":["trainer.train()\n","print(\"Fine-tuning complete.\")"],"metadata":{"id":"c7MraJ2nd0m_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 11. Loss Curves"],"metadata":{"id":"zjgMBwjcnRIT"}},{"cell_type":"code","source":["log_file = os.path.join(training_args.output_dir, \"checkpoint-XXX\", \"trainer_state.json\")\n","# Note: You may need to update 'checkpoint-XXX' to the latest checkpoint folder\n","# For simplicity, we'll access the trainer's state directly.\n","\n","logs = trainer.state.log_history\n","\n","train_steps = [x[\"step\"] for x in logs if \"loss\" in x]\n","train_loss = [x[\"loss\"] for x in logs if \"loss\" in x]\n","\n","eval_steps = [x[\"step\"] for x in logs if \"eval_loss\" in x]\n","eval_loss = [x[\"eval_loss\"] for x in logs if \"eval_loss\" in x]\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(train_steps, train_loss, label=\"Train Loss\")\n","plt.plot(eval_steps, eval_loss, label=\"Eval Loss\", marker='o')\n","plt.title(\"Training and Evaluation Loss\")\n","plt.xlabel(\"Steps\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"nBWwMBgBd0p3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ‚úÖ 12. Final Evaluation and Metrics Summary"],"metadata":{"id":"8pLr8FmZnOlE"}},{"cell_type":"code","source":["# Use the fine-tuned model (already loaded in 'trainer.model')\n","model = trainer.model\n","test_predictions = []\n","test_ground_truth = []\n","\n","# Loop through the test set\n","print(\"Running final evaluation on test set...\")\n","for example in tqdm(ds_test):\n","    # Re-create the prompt, but without the answer\n","    prompt_text = (\n","        f\"Analyze the following meme components and classify it. \"\n","        f\"Text: {example['text']}\\n\"\n","        f\"Description: {example['description']}\\n\\n\"\n","        f\"What is the category? \"\n","    )\n","    image = Image.open(example['image_path']).convert(\"RGB\")\n","    messages = [{\"role\": \"user\", \"content\": [image, prompt_text]}]\n","\n","    # Prepare input\n","    prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n","    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n","\n","    # Generate response\n","    output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n","    generated_text = processor.decode(output[0], skip_special_tokens=True)\n","\n","    # Extract just the answer (the label)\n","    prediction = generated_text.split(\"assistant\\n\")[-1].strip()\n","\n","    test_predictions.append(prediction)\n","    test_ground_truth.append(example['label'])\n","\n","# --- Compute Metrics ---\n","# Filter out any predictions that were not valid labels\n","valid_preds = [p for p in test_predictions if p in label_to_id]\n","valid_gt = [g for p, g in zip(test_predictions, test_ground_truth) if p in label_to_id]\n","\n","print(f\"\\n{len(valid_preds)} / {len(test_predictions)} predictions were valid labels.\")\n","\n","print(\"\\nüìä Final Test Set Results:\")\n","print(classification_report(valid_gt, valid_preds, target_names=label_to_id.keys()))\n","\n","# Manual calculation for overall results\n","results = {}\n","results[\"accuracy\"] = accuracy_score(valid_gt, valid_preds)\n","results[\"f1_weighted\"] = f1_score(valid_gt, valid_preds, average=\"weighted\")\n","results[\"precision_weighted\"] = precision_score(valid_gt, valid_preds, average=\"weighted\", zero_division=0)\n","results[\"recall_weighted\"] = recall_score(valid_gt, valid_preds, average=\"weighted\", zero_division=0)\n","\n","print(\"\\nüìä Summary Metrics:\")\n","for k, v in results.items():\n","    print(f\"{k:20s}: {v:.4f}\")"],"metadata":{"id":"KlaHEt12nO9f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üíæ 13. Save Final Model"],"metadata":{"id":"0KwfLD7CnPCV"}},{"cell_type":"code","source":["# Save the LoRA adapters\n","model.save_pretrained(OUTPUT_DIR)\n","# Save the processor\n","processor.save_pretrained(OUTPUT_DIR)\n","\n","print(f\"\\n‚úÖ Fine-tuning complete. Model saved to: {OUTPUT_DIR}\")"],"metadata":{"id":"RV699iGtnPNf"},"execution_count":null,"outputs":[]}]}